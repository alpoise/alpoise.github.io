---
layout: post
title: "AlexNet: oxflowers 17"
categories:
  - Deep Learning
tags:
  - alignment
  - content
  - css
  - markup
---

* content
{:toc}


AlexNet作为CNN的一类，本文是结合Tensorflow在oxflowers数据上对其的实现。
# 数据读入

本部分目的是实现oxflowers的数据读入，并分割为训练集和测试集。从如下[数据连接](http://www.robots.ox.ac.uk/~vgg/data/flowers/17)下载源数据并阅读数据说明。数据集包含1360张花朵的图片，按顺序每80个为一类，总计17类。因此首先要对数据集分割并加上标签，之后用PIL包的Image函数将图片读为张量，根据数据集中的datasplits.mat将数据集分割为训练集和样本集，最后定义shuffle的规则方便分批训练。代码LoadData.py如下：

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Apr 23 12:56:57 2018

@author: Alpoise
"""

import os
import numpy as np
import scipy.io as sio
from PIL import Image

os.chdir('/Users/Alpoise/Desktop/DL_course/HW2')
dir='oxflower17/jpg/'

def to_categorical(y, nb_classes): #把label改成one_hot向量
    y = np.asarray(y, dtype='int32')
    if not nb_classes:
        nb_classes = np.max(y)+1
    Y = np.zeros((1, nb_classes))
    Y[0,y] = 1.
    return Y

def build_class_directories(dir):
    dir_id = 0
    class_dir = os.path.join(dir, str(dir_id))
    if not os.path.exists(class_dir):
        os.mkdir(class_dir)
    for i in range(1, 1361):
        fname = "image_" + ("%.4d" % i) + ".jpg"
        #%i改为了%d    
        os.rename(os.path.join(dir, fname), os.path.join(class_dir, fname))
        #把文件的位置重命名即意味着挪动了位置，比复制快
        if i % 80 == 0 and dir_id < 16:
            dir_id += 1
            class_dir = os.path.join(dir, str(dir_id))
            os.mkdir(class_dir)

def get_input(resize=[224,224]):
    print('Load data...')

    getJPG = lambda filePath: np.array(Image.open(filePath).resize(resize))
    #lamda简洁地定义函数的方式，冒号前面是自变量，后面是返回值
    #resize使得图片长宽统一，同时并不损失RGB值，参见Image包的用法
    dataSet=[];labels=[];choose=1
    classes = os.listdir(dir)
    for index, name in enumerate(classes):
        #enumerate将一个可遍历的数据对象组合为索引序列，同时列出数据和数据下标
        class_path = dir+ name + "/"
        if os.path.isdir(class_path): #只有分成组的图片才构成文件夹，才是一个路径
            for img_name in os.listdir(class_path):
                img_path = class_path + img_name #得到每张图的路径
                img_raw = getJPG(img_path) #读入每张图
                dataSet.append(img_raw) #append向列表尾部加一个元素
                y = to_categorical(int(name),17) #标记成one_hot label
                labels.append(y)

    datasplits = sio.loadmat('oxflower17/datasplits.mat')
    keys = [x + str(choose) for x in ['trn','val','tst']]
    #前面定义过choose=1，这里即选了datasplits的1子集
    train_set, vall_set, test_set = [set(list(datasplits[name][0])) for name in keys]
    #set函数创建成集合的形式，可并交补
    train_data, train_label,test_data ,test_label= [],[],[],[]

    for i in range(len(labels)):
        num = i + 1
        if num in test_set:
            test_data.append(dataSet[i])
            test_label.extend(labels[i])
        else:
            train_data.append(dataSet[i])
            train_label.extend(labels[i])
        #把索引在test_set的分到测试集，否则分到训练集
    train_data = np.array(train_data, dtype='float32')
    train_label = np.array(train_label, dtype='float32')
    test_data = np.array(test_data, dtype='float32')
    test_label = np.array(test_label, dtype='float32')
    return train_data, train_label,test_data ,test_label

def batch_iter(data, batch_size, num_epochs, shuffle=True):
    #1个epoch等于使用训练集中的全部样本训练一次
    data = np.array(data)
    data_size = len(data)
    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1
    #重要！说明对每次epoch末尾不足一个batchsize的也算作一个batch，怎么算的见下文end_index处
    for epoch in range(num_epochs):
        # 每个epoch把数据打乱下顺序
        if shuffle:
            shuffle_indices = np.random.permutation(np.arange(data_size))
            shuffled_data = data[shuffle_indices]
        else:
            shuffled_data = data
        for batch_num in range(num_batches_per_epoch):
            start_index = batch_num * batch_size
            end_index = min((batch_num + 1) * batch_size, data_size)
            yield shuffled_data[start_index:end_index]
            #yield就是 return 返回一个值，并且记住这个返回的位置，下次迭代就从这个位置后开始

if __name__=='__main__':
    #build_class_directories(os.path.join(dir))
    #首次没有分类好的话运行此函数分类
    train_data, train_label,test_data, test_label=get_input()
    print(len(train_data),len(test_data))
#当模块被直接运行时，__name__ == '__main__'以下代码块将被运行，当模块是被导入时，代码块不被运行
```
# AlexNet
本部分介绍AlexNet网络的结构。AlexNet的详细分析可以参见这篇[博客](https://blog.csdn.net/zyqdragon/article/details/72353420)，作为卷积神经网络的一种，它首先用三层“卷积—ReLU— 池化— LRN”的结构提取特征，其中Local Response Normalization (LRN)是[AlexNet](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)2012年首次提出来的，然后再搭建三个全连接层得到输出，具体结构见下图。

![AlexNet](/Users/Alpoise/Documents/GitHub/alpoise.github.io/_plugins/jpg/AlexNet.png)

注意原始图的最终输出意味着1000个标签，且网络在提取特征的时候都是分割成两两部分便于在两个GPU上计算，本文除了不做分割其余保持一致。代码AlexNet.py如下：
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Apr 23 12:55:45 2018

@author: Alpoise
"""

import tensorflow as tf

def conv(input,fh,fw,n_out,name):
    #第一层卷积用此函数，区别在于stride为4
    n_in = input.get_shape()[-1].value
    with tf.name_scope(name) as scope:
        filter = tf.get_variable(scope+"w", shape=[fh, fw, n_in, n_out], dtype=tf.float32,
                                 initializer=tf.contrib.layers.xavier_initializer_conv2d())
        #变量声明和初始化都打包在函数内
        conv = tf.nn.conv2d(input, filter, [1,4,4,1], padding='SAME')
        bias_val = tf.constant(0.0, shape=[n_out], dtype=tf.float32)
        biases = tf.Variable(bias_val, trainable=True, name='b')
        z = tf.nn.bias_add(conv, biases)
        relu = tf.nn.relu(z, name=scope)
        return relu

def conv_layer(input,fh,fw,n_out,name):
    #对隐藏层不使用stride
    n_in = input.get_shape()[-1].value
    with tf.name_scope(name) as scope:
        filter = tf.get_variable(scope+"w", shape=[fh, fw, n_in, n_out], dtype=tf.float32,
                                 initializer=tf.contrib.layers.xavier_initializer_conv2d())
        conv = tf.nn.conv2d(input, filter, [1,1,1,1], padding='SAME')
        bias_val = tf.constant(0.0, shape=[n_out], dtype=tf.float32)
        biases = tf.Variable(bias_val, trainable=True, name='b')
        z = tf.nn.bias_add(conv, biases)
        relu = tf.nn.relu(z, name=scope)
        return relu

def full_connected_layer(input,n_in,n_out,name):
    with tf.name_scope(name) as scope:
        filter = tf.get_variable(scope+"w", shape=[n_in, n_out], dtype=tf.float32,
                                 initializer=tf.contrib.layers.xavier_initializer_conv2d())
        bias_val = tf.constant(0.1, shape=[n_out], dtype=tf.float32)
        biases = tf.Variable(bias_val, name='b')
        fc = tf.nn.relu(tf.matmul(input,filter)+biases)
        return fc

def max_pool_layer(input,name):
    return tf.nn.max_pool(input, ksize=[1,3,3,1], strides=[1,2,2,1],padding='SAME', name=name)

def norm(name, l_input, lsize=4):
    return tf.nn.lrn(l_input, lsize, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=name)

class CNN(object):
    def __init__(
      self, num_classes,l2_reg_lambda=0.01):

        self.input_x = tf.placeholder(tf.float32, [None, 224,224,3], name="input_x")
        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name="input_y")
        self.dropout_keep_prob = tf.placeholder(tf.float32, name="dropout_keep_prob")
        l2_loss = tf.constant(0.1)

        with tf.name_scope("conv-maxpool"):
            # First layer
            self.conv1_1 = conv(self.input_x,fh=11, fw=11,n_out=96,name="conv1_1")
            self.pool1 = max_pool_layer(self.conv1_1,name='pool1')
            self.norm1 = norm('norm1', self.pool1, lsize=4)
            self.norm1 = tf.nn.dropout(self.norm1, self.dropout_keep_prob)

            # Second layer
            self.conv2_1 = conv_layer(self.norm1, fh=5, fw=5, n_out=192,name='conv2_1')
            self.pool2 = max_pool_layer(self.conv2_1, name='pool2')
            self.norm2 = norm('norm2', self.pool2, lsize=4)
            self.norm2 = tf.nn.dropout(self.norm2, self.dropout_keep_prob)

            # Third layer
            self.conv3_1 = conv_layer(self.norm2, fh=3, fw=3, n_out=384, name='conv3_1')
            self.conv3_2 = conv_layer(self.conv3_1,fh=3, fw=3, n_out=384, name='conv3_2')
            self.conv3_3 = conv_layer(self.conv3_2, fh=3, fw=3, n_out=256, name='conv3_3')
            self.pool3 = max_pool_layer(self.conv3_3, name='pool3')
            self.norm3 = norm('self.norm3', self.pool3, lsize=4)
            self.norm3 = tf.nn.dropout(self.norm3,self.dropout_keep_prob)
            
        # Dense Layer <L2 for MLP>
        with tf.name_scope("output"):
            shape = self.norm3.get_shape().as_list()
            self.dim = 1
            for d in shape[1:]:
                self.dim *= d
            self.h_pool_flat= tf.reshape(self.norm3, [-1, self.dim])
            
            self.fc1 = full_connected_layer(self.h_pool_flat,n_in=self.dim,n_out = 4096,name = "fc1")
            self.fc2 = full_connected_layer(self.fc1,n_in = 4096,n_out = 1000,name = "fc2")
            self.fc3 = full_connected_layer(self.fc2,n_in = 1000,n_out = 17,name = "fc3")
            
            self.out = self.fc3

        self.loss, l2_loss = 0,0

        with tf.name_scope("loss"):
            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.out, labels=self.input_y)
            self.loss = tf.reduce_mean(losses)+l2_reg_lambda*l2_loss

        # Accuracy
        self.accuracy=0
        with tf.name_scope("accuracy"):
            correct_predictions = tf.equal(tf.argmax(self.out,1), tf.argmax(self.input_y, 1))
            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"), name="accuracy")
```

本网络用到了CNN中的stride，padding，pooling，dropout等概念，此外一个技巧是在训练MLP加入L2惩罚有助于提高精度，如Tensorlayer[说明文档](https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_cifar10.py)中有提到但由于对tensorflow变量命名管理还有所欠缺目前还没有对MLP加入l2惩罚。

# 训练

网络的训练并没什么特别的，代码AlexTrain.py如下：

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed May  2 20:03:38 2018

@author: Alpoise
"""

import tensorflow as tf
import os

os.chdir('/Users/Alpoise/Desktop/DL_course/HW2')

import datetime
import loadData
from AlexNet import CNN
# Parameters
# ==================================================

# 模型超参
tf.flags.DEFINE_float("dropout_keep_prob", 0.5, "Dropout keep probability (default: 0.5)")
tf.flags.DEFINE_float("l2_reg_lambda", 0.01, "L2 regularization lambda (default: 0.01)")

# 训练参数
tf.flags.DEFINE_integer("batch_size", 30, "Batch Size (default: 30)")
tf.flags.DEFINE_integer("num_epochs", 20, "Number of training epochs (default: 20)")
tf.flags.DEFINE_integer("evaluate_every", 20, "Evaluate model on dev set after this many steps (default: 20)")

FLAGS = tf.flags.FLAGS
FLAGS._parse_flags()
print("\nParameters:")
for attr, value in sorted(FLAGS.__flags.items()):
    print("{}={}".format(attr.upper(), value))
print("")

# 数据载入
train_x, train_y, test_x, test_y = loadData.get_input()
print("Train/Test: {:d}/{:d}".format(len(train_y), len(test_y)))

# 训练
# ==================================================
with tf.Graph().as_default():
    sess = tf.Session()
    with sess.as_default():
        cnn =CNN(
            num_classes=17,
            l2_reg_lambda=FLAGS.l2_reg_lambda)

        # 定义训练op
        global_step = tf.Variable(0, name="global_step", trainable=False)
        optimizer = tf.train.AdamOptimizer(1e-4)
        grads_and_vars = optimizer.compute_gradients(cnn.loss)
        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)
        
        # 变量初始化
        sess.run(tf.global_variables_initializer())

        def train_step(x_batch, y_batch):
            feed_dict = {
              cnn.input_x: x_batch,
              cnn.input_y: y_batch,
              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob
            }
            _, step, loss, accuracy = sess.run(
                [train_op, global_step,cnn.loss, cnn.accuracy],
                feed_dict)
            time_str = datetime.datetime.now().isoformat()
            print("{}: step {}, loss {:g}, acc {:g}".format(time_str, step, loss, accuracy))

        def test_step(x_batch, y_batch):
            feed_dict = {
              cnn.input_x: x_batch,
              cnn.input_y: y_batch,
              cnn.dropout_keep_prob: 1.0
            }
            step,loss, accuracy = sess.run(
                [global_step,cnn.loss, cnn.accuracy],
                feed_dict)
            time_str = datetime.datetime.now().isoformat()
            print("{}: step {}, loss {:g}, acc {:g}".format(time_str, step, loss, accuracy))

        # 生成小batch
        batches =loadData.batch_iter(
            list(zip(train_x,train_y)), FLAGS.batch_size, FLAGS.num_epochs)
        # 对每个batch循环：
        for batch in batches:
            x_batch, y_batch = zip(*batch)
            train_step(x_batch, y_batch)
            current_step = tf.train.global_step(sess, global_step)
            if current_step % FLAGS.evaluate_every == 0:
                print("\nEvaluation:")
                test_step(test_x, test_y)
            
```



